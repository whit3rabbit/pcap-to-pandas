{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scapy\n",
    "#!pip install pandas\n",
    "#!pip install networkx\n",
    "#!pip install polars\n",
    "#!pip install aspose-diagram-python \n",
    "#!pip install matplotlib\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import * # Packet manipulation\n",
    "import polars as pl # Pandas - Create and Manipulate DataFrames\n",
    "from datetime import datetime # Datetime - Convert Epoch to Datetime\n",
    "import ipaddress # IPAddress - Check for multicast and broadcast addresses\n",
    "import time # Measure time it takes to run\n",
    "import csv # CSV - Write to CSV\n",
    "import re # Regex for name generation of files\n",
    "import queue # Queue - Used for threading\n",
    "from threading import Thread # Threading - Run multiple functions at once\n",
    "import networkx as nx # NetworkX - Create and Manipulate Graphs\n",
    "import os # OS - Check if file exists\n",
    "import matplotlib.pyplot as plt # Matplotlib - Plot Graphs\n",
    "import numpy as np # Numpy - Create Arrays\n",
    "import spacy # Spacy - NLP\n",
    "#import aspose.diagram\n",
    "#from aspose.diagram import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcap_name = \"The Ultimate PCAP v20221220.pcapng\"\n",
    "csv_file_name = re.sub(r'[^\\w\\s]', '', pcap_name).replace(\" \", \"_\") + \".csv\"\n",
    "parquet_file_name = re.sub(r'[^\\w\\s]', '', pcap_name).replace(\" \", \"_\") + \".parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_to_numbers = {'hopopt': 0, 'icmp': 1, 'igmp': 2, 'ggp': 3, 'ipv4': 4, 'st': 5, 'tcp': 6, 'cbt': 7, \n",
    "    'egp': 8, 'igp': 9, 'bbn-rcc-mon': 10, 'nvp-ii': 11, 'pup': 12, 'emcon': 14, 'xnet': 15, 'chaos': 16, 'udp': 17, 'mux': 18,\n",
    "    'dcn-meas': 19, 'hmp': 20, 'prm': 21, 'xns-idp': 22, 'trunk-1': 23, 'trunk-2': 24, 'leaf-1': 25, 'leaf-2': 26, 'rdp': 27, \n",
    "    'irtp': 28, 'iso-tp4': 29, 'netblt': 30, 'mfe-nsp': 31, 'merit-inp': 32, 'dccp': 33, '3pc': 34, 'idpr': 35, 'xtp': 36, \n",
    "    'ddp': 37, 'idpr-cmtp': 38, 'tp++': 39, 'il': 40, 'ipv6': 41, 'sdrp': 42, 'ipv6-route': 43, 'ipv6-frag': 44, 'idrp': 45,\n",
    "    'rsvp': 46, 'gre': 47, 'dsr': 48, 'bna': 49, 'esp': 50, 'ah': 51, 'i-nlsp': 52, 'narp': 54, 'mobile': 55, 'tlsp': 56, \n",
    "    'skip': 57, 'ipv6-icmp': 58, 'ipv6-nonxt': 59, 'ipv6-opts': 60, 'cftp': 62, 'sat-expak': 64, 'kryptolan': 65, 'rvd': 66,\n",
    "    'ippc': 67, 'sat-mon': 69, 'visa': 70, 'ipcv': 71, 'cpnx': 72, 'cphb': 73, 'wsn': 74, 'pvp': 75, 'br-sat-mon': 76, \n",
    "    'sun-nd': 77, 'wb-mon': 78, 'wb-expak': 79, 'iso-ip': 80, 'vmtp': 81, 'secure-vmtp': 82, 'vines': 83, 'ttp': 84,\n",
    "    'iptm': 84, 'nsfnet-igp': 85, 'dgp': 86, 'tcf': 87, 'eigrp': 88, 'ospfigp': 89, 'sprite-rpc': 90,\n",
    "    'larp': 91, 'mtp': 92, 'ax.25': 93, 'ipip': 94, 'scc-sp': 96, 'etherip': 97, 'encap': 98, 'gmtp': 100, \n",
    "    'ifmp': 101, 'pnni': 102, 'pim': 103, 'aris': 104, 'scps': 105, 'qnx': 106, 'a/n': 107, 'ipcomp': 108, \n",
    "    'snp': 109, 'compaq-peer': 110, 'ipx-in-ip': 111, 'vrrp': 112, 'pgm': 113, 'l2tp': 115, 'ddx': 116, 'iatp': 117, \n",
    "    'stp': 118, 'srp': 119, 'uti': 120, 'smp': 121, 'ptp': 123, 'fire': 125, 'crtp': 126, 'crudp': 127, 'sscopmce': 128, \n",
    "    'iplt': 129, 'sps': 130, 'pipe': 131, 'sctp': 132, 'fc': 133, 'rsvp-e2e-ignore': 134, 'udplite': 136, 'mpls-in-ip': 137,\n",
    "      'manet': 138, 'hip': 139, 'shim6': 140, 'wesp': 141, 'rohc': 142, 'ethernet': 143, 'aggfrag': 144, 'rsvp-e2e': 145}\n",
    "\n",
    "# https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml\n",
    "\n",
    "def get_protocol_name(protocol_number):\n",
    "    for protocol_name, number in protocol_to_numbers.items():\n",
    "        if number == protocol_number:\n",
    "            return protocol_name\n",
    "    return protocol_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAPToDataFrame:\n",
    "    def __init__(self, pcap_name, parquet_file_name, csv_file_name):\n",
    "        self.capture_file = pcap_name\n",
    "        self.parquet_file = parquet_file_name\n",
    "        \n",
    "        # Create new CSV file and remove old one\n",
    "        self.csv_file_name = csv_file_name\n",
    "        self.headers = [\"time\",\"src_ip\",\"src_mac\",\"dst_ip\",\"dst_mac\",\"protocol\",\"payload_size\",\n",
    "                        \"multicast\",\"private_to_private\",\"dst_broadcast\",\"src_port\",\"dst_port\",\n",
    "                        \"localhost\"]\n",
    "        if os.path.exists(self.csv_file_name):\n",
    "            os.remove(self.csv_file_name)\n",
    "\n",
    "        self.queue = []\n",
    "        self.batch_size = 5000  # Number of packets to buffer before writing to file\n",
    "\n",
    "    def write_to_csv(self):\n",
    "        with open(self.csv_file_name, 'a') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.headers)\n",
    "            writer.writeheader()\n",
    "            while self.queue:\n",
    "                data = self.queue.pop(0)\n",
    "                if data is None:\n",
    "                    break\n",
    "                writer.writerow(data)\n",
    "\n",
    "    # Check if ip is multicast and private>private and broadcast\n",
    "    def check_multicast_and_private(self, packet, data):\n",
    "        src_ip = ipaddress.ip_address(packet[IP].src)\n",
    "        dst_ip = ipaddress.ip_address(packet[IP].dst)\n",
    "\n",
    "        data[\"multicast\"] = src_ip.is_multicast or dst_ip.is_multicast\n",
    "        data[\"private_to_private\"] = src_ip.is_private and dst_ip.is_private\n",
    "        data[\"localhost\"] = src_ip.is_loopback or dst_ip.is_loopback\n",
    "        data[\"dst_broadcast\"] = src_ip.is_global or dst_ip.is_global\n",
    "\n",
    "    # Extract the port numbers\n",
    "    def extract_port_numbers(self, packet, data):\n",
    "        if packet.haslayer(TCP):\n",
    "            data[\"src_port\"] = int(packet[TCP].sport)\n",
    "            data[\"dst_port\"] = int(packet[TCP].dport)\n",
    "        elif packet.haslayer(UDP):\n",
    "            data[\"src_port\"] = int(packet[UDP].sport)\n",
    "            data[\"dst_port\"] = int(packet[UDP].dport)\n",
    "\n",
    "    # Extract the conversations from the packet\n",
    "    def conversations_extract(self, packet):\n",
    "\n",
    "        # Check if the packet has the IP and Ethernet layers\n",
    "        if not packet.haslayer(IP) or not packet.haslayer(Ether):\n",
    "            return\n",
    "        \n",
    "        # Convert the timestamp to a readable UTC time\n",
    "        #time = datetime.utcfromtimestamp(int(packet.time)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        time = datetime.utcfromtimestamp(int(packet.time)).isoformat()\n",
    "        protocol = get_protocol_name(int(packet[IP].proto))\n",
    "\n",
    "        # Extract the desired data from the packet\n",
    "        data = {\n",
    "            \"time\": time,\n",
    "            \"src_ip\": packet[IP].src,\n",
    "            \"src_mac\": packet[Ether].src,\n",
    "            \"dst_ip\": packet[IP].dst,\n",
    "            \"dst_mac\": packet[Ether].dst,\n",
    "            \"protocol\": protocol,\n",
    "            \"payload_size\": len(packet[IP].payload)\n",
    "        }\n",
    "\n",
    "        # Extract\n",
    "        self.check_multicast_and_private(packet, data)\n",
    "        self.extract_port_numbers(packet, data)\n",
    "\n",
    "        # Add the data to the queue\n",
    "        self.queue.append(data)\n",
    "\n",
    "        # If the queue is full, write the data to the CSV file\n",
    "        if len(self.queue) >= self.batch_size:\n",
    "            self.write_to_csv()\n",
    "\n",
    "    def read_pcap_to_dataframe(self):\n",
    "        # Start time to read pcap time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Read the PCAP file and extract the data from each packet using the conversations_extract function\n",
    "        # Use a queue to write the data to a CSV file      \n",
    "        for packet in PcapReader(self.capture_file):\n",
    "            self.conversations_extract(packet)\n",
    "        self.write_to_csv()  # Write any remaining data to the file\n",
    "\n",
    "        # Remove any previous parquet file\n",
    "        if os.path.exists(self.parquet_file):\n",
    "            os.remove(self.parquet_file)\n",
    "\n",
    "        # Open the CSV file and convert it to a Polars dataframe\n",
    "        conversations_df = pl.read_csv(self.csv_file_name, ignore_errors=True, parse_dates=True, infer_schema_length=10)\n",
    "        conversations_df.write_parquet(self.parquet_file, compression='brotli')\n",
    "\n",
    "        # Remove the CSV file\n",
    "        if os.path.exists(self.csv_file_name):\n",
    "            os.remove(self.csv_file_name)\n",
    "        '''\n",
    "        File size differences using test 12mb pcap file:\n",
    "        12M  | pcapng\n",
    "        2.6M | csv\n",
    "        65K | parquet\n",
    "        '''\n",
    "        # Record time taken to process the PCAP file\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Elapsed time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the PCAPToDataFrame class\n",
    "pcap_to_df = PCAPToDataFrame(pcap_name, parquet_file_name, csv_file_name)\n",
    "\n",
    "# Read the PCAP file and create a csv\n",
    "pcap_to_df.read_pcap_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(parquet_file_name)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter((pl.col(\"dst_ip\") != \"255.255.255.255\") & (pl.col(\"dst_ip\") != \"0.0.0.0\"))\n",
    "filtered_df = filtered_df.filter((pl.col(\"localhost\") == False))\n",
    "filtered_df = filtered_df.filter((pl.col(\"private_to_private\") == True) & (pl.col(\"multicast\") == False))\n",
    "filtered_df = filtered_df.drop([\"time\", \"protocol\", \"payload_size\", \"multicast\", \"private_to_private\"])\n",
    "filtered_df = filtered_df.unique(subset=[\"src_ip\",\"dst_ip\"])\n",
    "filtered_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_mac_dict = {}\n",
    "for row in filtered_df.iterrows(named=True):\n",
    "    ip_mac_dict[row.src_ip] = row.src_mac\n",
    "\n",
    "# Create a graph from the dataframe\n",
    "G = nx.Graph() \n",
    "\n",
    "# From the filtered_df dataframe, create an edge with src_ip and dst_ip with src_ip as label\n",
    "for row in filtered_df.iterrows(named=True):\n",
    "    G.add_node(row.src_ip, label=row.src_ip, nodeid=ip_mac_dict[row.src_ip])\n",
    "    G.add_node(row.dst_ip, label=row.dst_ip)\n",
    "    G.add_edge(row.src_ip, row.dst_ip, label=row.src_ip)\n",
    "    \n",
    "nx.draw_planar(G, with_labels = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create a new Visio document\n",
    "visio_doc = Diagram()\n",
    "\n",
    "# TODO: Add stencil file\n",
    "# Load stencil file\n",
    "diagram = Diagram(\"Basic-Shapes.vss\")\n",
    "\n",
    "# Add shapes to the document\n",
    "for node in G.nodes():\n",
    "    shape = visio_doc.add_shape(4.25, 5.5, 2, 1, \"Rectangle\", 0)\n",
    "    shape.text.value = node\n",
    "\n",
    "# Create connections between shapes\n",
    "for edge in G.edges():\n",
    "    visio_doc.add_connector(edge[0], edge[1])\n",
    "\n",
    "# Save the Visio document\n",
    "visio_doc.save(\"network_diagram.vsdx\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://raw.githubusercontent.com/chmduquesne/python-drawio/main/drawio/drawio.py\n",
    "\n",
    "def max_degree(g):\n",
    "    \"\"\"\n",
    "    Returns the maximum number of edges of any node in the graph\n",
    "    \"\"\"\n",
    "    return max([g.degree[n] for n in g])\n",
    "\n",
    "\n",
    "\n",
    "def edge_styles(g):\n",
    "    \"\"\"\n",
    "    Returns the list of all possible edge styles\n",
    "    \"\"\"\n",
    "    styles = set()\n",
    "    for e in g.edges(data=\"style\"):\n",
    "        style = \"-\"\n",
    "        if e[2] is not None:\n",
    "            style = e[2]\n",
    "        styles.add(style)\n",
    "    return sorted(list(styles))\n",
    "\n",
    "\n",
    "\n",
    "def write_header(g, f):\n",
    "    \"\"\"\n",
    "    Creates a header for the csv file of the graph\n",
    "    \"\"\"\n",
    "    n = max_degree(g)\n",
    "    es = edge_styles(g)\n",
    "\n",
    "    f.write(\"# identity: nodeid\\n\")\n",
    "    f.write(\"# label: %label%\\n\")\n",
    "    f.write(\"# style: %style%\\n\")\n",
    "    f.write(\"# link: url\\n\")\n",
    "    f.write(\"# width: @width\\n\")\n",
    "    f.write(\"# height: @height\\n\")\n",
    "    f.write(\"# layout: verticalflow\\n\")\n",
    "\n",
    "    refs = [f\"ref_{i}_{j}\" for j in range(len(es)) for i in range(n)]\n",
    "    labels = [f\"label_{i}\" for i in range(n)]\n",
    "\n",
    "    f.write(\"# ignore: nodeid,style,height,width,\" + \",\".join(refs +\n",
    "        labels) + \"\\n\" )\n",
    "\n",
    "    # ref_i_j is connected to nodeid with label i and edge style j\n",
    "    for j, s in enumerate(es):\n",
    "        for i in range(n):\n",
    "            f.write(\n",
    "                f'# connect: {{\"from\": \"ref_{i}_{j}\", \"to\": \"nodeid\", '\n",
    "                f'\"fromlabel\": \"label_{i}\", '\n",
    "                f'\"style\": \"{s}\"}}\\n')\n",
    "    f.write(','.join(\n",
    "        [\"nodeid\", \"label\", \"tags\", \"style\", \"width\", \"height\", \"link\"] +\n",
    "        refs + labels\n",
    "        ) + \"\\n\" )\n",
    "\n",
    "def write_graph(g, f, ip_mac_dict):\n",
    "    \"\"\"\n",
    "    Creates the content for the csv file of the graph\n",
    "    \"\"\"\n",
    "    n = max_degree(g)\n",
    "    es = edge_styles(g)\n",
    "\n",
    "    for node in g.nodes:\n",
    "        label = g.nodes[node].get(\"label\", \"-\")\n",
    "        tags = g.nodes[node].get(\"tags\", \"-\")\n",
    "        style = g.nodes[node].get(\"style\", \"-\")\n",
    "        link = g.nodes[node].get(\"link\", \"-\")\n",
    "        width = g.nodes[node].get(\"width\", \"auto\")\n",
    "        height = g.nodes[node].get(\"height\", \"auto\")\n",
    "\n",
    "        # ref_i_j is connected to nodeid with label i and edge style j\n",
    "        refs = [\"-\"] * n * len(es)\n",
    "        labels = [\"-\"] * n\n",
    "        for i, e in enumerate(g.edges(node, data=True)):\n",
    "            data = e[2]\n",
    "            j = es.index(data.get(\"style\", \"-\"))\n",
    "            refs[j*n+i] = f\"{e[1]}\"\n",
    "            labels[i] = data.get(\"label\", \"-\")\n",
    "\n",
    "        # Add the nodeid using the ip_mac_dict\n",
    "        nodeid = ip_mac_dict.get(node, \"-\")\n",
    "\n",
    "        f.write(','.join([f\"{node}\", label, tags, style, width, height, link, nodeid] + refs + labels) + \"\\n\")\n",
    "\n",
    "def write(g, f):\n",
    "    write_header(g, f)\n",
    "    write_graph(g, sys.stdout, ip_mac_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw.io CSV format for input\n",
    "write(G, sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a csv file\n",
    "class parse_http:\n",
    "    def __init__(self, pcap_file):\n",
    "        self.pcap_file = pcap_file\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def extract_sensitive_info(self, url):\n",
    "        # Extract key-value pairs from URL\n",
    "        match = re.search(r'([^?=&]+)(=([^&]*))?', url)\n",
    "        if match:\n",
    "            pairs = match.groups()\n",
    "            sensitive_info = []\n",
    "            for i in range(0, len(pairs), 3):\n",
    "                key = pairs[i]\n",
    "                value = pairs[i+2] if pairs[i+2] else \"\"\n",
    "                doc = self.nlp(value)\n",
    "                for ent in doc.ents:\n",
    "                    sensitive_info.append((key, ent.text))\n",
    "            return sensitive_info\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def extract_info(self):\n",
    "        # Read in pcap file using Scapy\n",
    "        packets = rdpcap(self.pcap_file)\n",
    "        data = []\n",
    "        # Iterate through packets and extract relevant information\n",
    "        for packet in packets:\n",
    "            if 'HTTP' in packet:\n",
    "                src_ip = packet[IP].src\n",
    "                dst_ip = packet[IP].dst\n",
    "                if packet.haslayer(TCP):\n",
    "                    src_port = packet[TCP].sport\n",
    "                    dst_port = packet[TCP].dport\n",
    "                    payload = packet[TCP].payload\n",
    "                elif packet.haslayer(UDP):\n",
    "                    src_port = packet[UDP].sport\n",
    "                    dst_port = packet[UDP].dport\n",
    "                    payload = packet[UDP].payload\n",
    "                if payload:\n",
    "                    payload = payload.decode()\n",
    "                # Extract URLs, referers, and user agents\n",
    "                urls = re.findall(r'(https?:\\/\\/[^\\s]+)', payload)\n",
    "                referers = re.findall(r'Referer: (.*)', payload)\n",
    "                user_agents = re.findall(r'User-Agent: (.*)', payload)\n",
    "                # Extract sensitive information from URLs\n",
    "                sensitive_info = self.extract_sensitive_info(urls[0]) if len(urls) > 0 else None\n",
    "                data.append([src_ip, dst_ip, src_port, dst_port, urls, referers, user_agents, sensitive_info]) \n",
    "        # Create DataFrame from extracted data\n",
    "        df = pl.DataFrame(data, columns=['src_ip', 'dst_ip', 'src_port', 'dst_port', 'urls', 'referers', 'user_agents', 'sensitive_info'])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_parser = parse_http(pcap_name)\n",
    "http_df = http_parser.extract_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of packets per protocol\n",
    "q = (\n",
    "    df.lazy()\n",
    "    .groupby(\"protocol\")\n",
    "    .agg(pl.count())\n",
    "    .sort(\"count\", reverse=True)\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "df_proto_top = q.collect()\n",
    "x = df_proto_top[\"protocol\"]\n",
    "y = df_proto_top[\"count\"]\n",
    "plt.bar(x, y)\n",
    "plt.xlabel('Protocol')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (\n",
    "    df.lazy()\n",
    "    .groupby([\"src_ip\",\"dst_ip\"])\n",
    "    .agg(pl.count())\n",
    "    .sort(\"count\", reverse=True)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "df_top_talkers = q.collect()\n",
    "x = df_top_talkers[\"src_ip\"] #or df[\"dst_ip\"]\n",
    "y = df_top_talkers[\"count\"]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xlabel('IP Address')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation = 90) # this will rotate the x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddc97ae128281c05480c1f8005c6b5489769cc6d6f9dc1223215b95e23507aa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
